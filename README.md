# Transformer from Scratch

Clean implementation of transformer architecture from first principles.

## Components

# Transformer from Scratch

Clean implementation of transformer architecture from first principles.

## Components

### Attention Mechanisms
- Scaled dot-product attention
- Multi-head attention (8 heads)
- Causal masking for autoregressive generation

### Transformer Block
- Feed-forward networks with expansion
- Layer normalization
- Residual connections
- Complete encoder stack (6 layers)

### Visualizations
- Attention weight heatmaps
- Multi-head pattern analysis
- Layer evolution tracking

## Architecture
Standard transformer encoder: 512d model, 8 heads, 2048d FFN, 6 layers
~25M parameters

## Status
ðŸ”„ Week 1 in progress (Days 1-3 complete)

## Structure
- `notebooks/` - Implementation notebooks
- `src/` - Production modules (coming soon)
