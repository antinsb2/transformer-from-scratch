# Learning Log

## Dec 21 - Attention Mechanism
- Implemented scaled dot-product attention
- Created visualization of attention weights
- Built reusable PyTorch module
- Key insight: Scaling by âˆšd_k prevents softmax saturation

## Next: Multi-head attention
